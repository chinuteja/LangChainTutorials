{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e85e3b",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a92b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from groq import Groq\n",
    "import os\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import PromptTemplate,load_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ab2608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7efa35b",
   "metadata": {},
   "source": [
    "## loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50da8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        model_name=\"Gemma2-9b-It\",\n",
    "        temperature=0 ## this is creative parameter\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b798631",
   "metadata": {},
   "source": [
    "## Prompt template example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82ddf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are greetings for Chinu in 5 languages:\n",
      "\n",
      "1. **English:** Hello, Chinu!\n",
      "2. **Spanish:** Hola, Chinu!\n",
      "3. **French:** Bonjour, Chinu!\n",
      "4. **German:** Hallo, Chinu!\n",
      "5. **Japanese:** こんにちは、チヌさん (Konnichiwa, Chinu-san) \n",
      "\n",
      "\n",
      "Let me know if you'd like greetings in other languages! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "template2 = PromptTemplate(\n",
    "    template='Greet this person in 5 languages. The name of the person is {name}',\n",
    "    input_variables=['name']\n",
    ")\n",
    "\n",
    "# fill the values of the placeholders\n",
    "prompt = template2.invoke({'name':'chinu'})\n",
    "\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36ec61",
   "metadata": {},
   "source": [
    "## Prompt Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cee9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
    "Explanation Style: {style_input}  \n",
    "Explanation Length: {length_input}  \n",
    "1. Mathematical Details:  \n",
    "   - Include relevant mathematical equations if present in the paper.  \n",
    "   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.  \n",
    "2. Analogies:  \n",
    "   - Use relatable analogies to simplify complex ideas.  \n",
    "If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.  \n",
    "Ensure the summary is clear, accurate, and aligned with the provided style and length.\n",
    "\"\"\",\n",
    "input_variables=['paper_input', 'style_input','length_input'],\n",
    "validate_template=True\n",
    ")\n",
    "\n",
    "template.save('template.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b21a8",
   "metadata": {},
   "source": [
    "## Prompt UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.header('Reasearch Tool')\n",
    "\n",
    "paper_input = st.selectbox( \"Select Research Paper Name\", [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers\", \"GPT-3: Language Models are Few-Shot Learners\", \"Diffusion Models Beat GANs on Image Synthesis\"] )\n",
    "\n",
    "style_input = st.selectbox( \"Select Explanation Style\", [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"] ) \n",
    "\n",
    "length_input = st.selectbox( \"Select Explanation Length\", [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"] )\n",
    "\n",
    "template = load_prompt('template.json') ## loading prompt template by running prompt_generator.py\n",
    "\n",
    "\n",
    "\n",
    "if st.button('Summarize'):\n",
    "    chain = template | model\n",
    "    result = chain.invoke({\n",
    "        'paper_input':paper_input,\n",
    "        'style_input':style_input,\n",
    "        'length_input':length_input\n",
    "    })\n",
    "    st.write(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40ee2d",
   "metadata": {},
   "source": [
    "## Chat bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_style = False\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "if old_style:\n",
    "    while True:\n",
    "        user_input = input('You: ')\n",
    "        chat_history.append(user_input) ## we are appending the every hunman response to the chat history\n",
    "        if user_input == 'exit':\n",
    "            break\n",
    "        result = model.invoke(chat_history) ## getting the result\n",
    "        chat_history.append(result.content)## now we are appending the AI response to the chat history\n",
    "        print(\"AI: \",result.content)\n",
    "\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(chat_history)\n",
    "else:\n",
    "    # creating a chat history\n",
    "    chat_history = [\n",
    "        SystemMessage(content='You are a helpful AI assistant')\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        user_input = input('You: ')\n",
    "        chat_history.append(HumanMessage(content=user_input)) ## we are appending the every hunman response to the chat history\n",
    "        if user_input == 'exit':\n",
    "            break\n",
    "        result = model.invoke(chat_history) ## getting the result\n",
    "        chat_history.append(AIMessage(content=result.content))## now we are appending the AI response to the chat history\n",
    "        print(\"AI: \",result.content)\n",
    "\n",
    "    print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b715a59",
   "metadata": {},
   "source": [
    "## Chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7f488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful cricket expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in simple terms, what is Dusra', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate([\n",
    "    SystemMessage(content='You are a helpful {domain} expert'),\n",
    "    HumanMessage(content='Explain in simple terms, what is {topic}')\n",
    "])\n",
    "##  output of the above template its not taking place hodlers\n",
    "# messages=[SystemMessage(content='You are a helpful {domain} expert', additional_kwargs={},\n",
    "#  response_metadata={}), HumanMessage(content='Explain in simple terms, what is {topic}',\n",
    "#   additional_kwargs={}, response_metadata={})]\n",
    "## passing system message like this to over come error\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful {domain} expert'),\n",
    "    ('human', 'Explain in simple terms, what is {topic}')\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({'domain':'cricket','topic':'Dusra'})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9437590",
   "metadata": {},
   "source": [
    "## Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8ce7022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about LangChain', additional_kwargs={}, response_metadata={}), AIMessage(content='Let\\'s talk about LangChain! \\n\\nLangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). Think of it as a toolbox filled with components specifically tailored for working with LLMs like me. \\n\\nHere\\'s a breakdown of what makes LangChain so powerful:\\n\\n**Key Features:**\\n\\n* **Modular Design:** LangChain breaks down LLM applications into smaller, manageable pieces called \"chains.\" These chains can be composed and chained together to create complex workflows.\\n\\n* **Variety of Components:** It offers a rich library of pre-built components:\\n    * **Models:**  Connectors to various LLMs (e.g., OpenAI, HuggingFace).\\n    * **Prompts:** Tools for crafting effective prompts to guide LLM responses.\\n    * **Memory:** Mechanisms to store and retrieve context across multiple interactions, making conversations more natural.\\n    * **Agents:**  Allow LLMs to interact with external tools and APIs, expanding their capabilities beyond text generation.\\n    * **Data Connections:**  Integrations with databases and other data sources to enable LLMs to access and process real-world information.\\n\\n* **Flexibility:** You can customize and extend LangChain to suit your specific needs.\\n\\n**Use Cases:**\\n\\nLangChain has a wide range of applications:\\n\\n* **Chatbots:** Building more sophisticated and context-aware chatbots.\\n* **Question Answering Systems:** Creating systems that can accurately answer questions based on a given context.\\n* **Text Summarization:**  Generating concise summaries of lengthy documents.\\n* **Code Generation:** Assisting developers by generating code snippets.\\n* **Personalized Learning:**  Tailoring educational experiences to individual learners.\\n\\n**Getting Started:**\\n\\nLangChain is relatively easy to learn and use.  \\n\\n1. **Installation:**  `pip install langchain`\\n2. **Documentation:**  The official documentation is excellent: [https://python.langchain.com/](https://python.langchain.com/)\\n\\nLet me know if you have any more questions about LangChain. I\\'m happy to provide more details or examples!\\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "model = ChatGroq(\n",
    "        groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "        model_name=\"Gemma2-9b-It\",\n",
    "        temperature=0.5 ## this is creative parameter\n",
    "    )\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content='You are a helpful assistant'),\n",
    "    HumanMessage(content='Tell me about LangChain')\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "messages.append(AIMessage(content=result.content))\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f4e92",
   "metadata": {},
   "source": [
    "## Message Place holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e66bc7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HumanMessage(content=\"I want to request a refund for my order #12345.\")\\n', 'AIMessage(content=\"Your refund request for order #12345 has been initiated. It will be processed in 3-5 business days.\")']\n",
      "messages=[SystemMessage(content='You are a helpful customer support agent', additional_kwargs={}, response_metadata={}), HumanMessage(content='HumanMessage(content=\"I want to request a refund for my order #12345.\")\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='AIMessage(content=\"Your refund request for order #12345 has been initiated. It will be processed in 3-5 business days.\")', additional_kwargs={}, response_metadata={}), HumanMessage(content='Where is my refund', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# chat template\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system','You are a helpful customer support agent'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'), ## we pass chat history of the customer here\n",
    "    ('human','{query}')\n",
    "])\n",
    "\n",
    "chat_history = []\n",
    "# load chat history\n",
    "with open('chat_history.txt') as f:\n",
    "    chat_history.extend(f.readlines())\n",
    "\n",
    "print(chat_history)\n",
    "\n",
    "# create prompt\n",
    "prompt = chat_template.invoke({'chat_history':chat_history, 'query':'Where is my refund'})\n",
    "\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
